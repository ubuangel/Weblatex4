@article{al2021speech,
  title={Speech emotion recognition based on SVM and KNN classifications fusion},
  author={Al Dujaili, Mohammed Jawad and Ebrahimi-Moghadam, Abbas and Fatlawi, Ahmed},
  journal={International Journal of Electrical and Computer Engineering},
  volume={11},
  number={2},
  pages={1259},
  year={2021},
  publisher={IAES Institute of Advanced Engineering and Science}
}

@ARTICLE{Aljuhani2021,
  author={Aljuhani, Reem Hamed and Alshutayri, Areej and Alahdal, Shahd},
  journal={IEEE Access}, 
  title={Arabic Speech Emotion Recognition From Saudi Dialect Corpus}, 
  year={2021},
  volume={9},
  number={},
  pages={127081-127085},
  doi={10.1109/ACCESS.2021.3110992}}

@INPROCEEDINGS{Umamaheswari2019,
  author={Umamaheswari, J. and Akila, A.},
  booktitle={2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)}, 
  title={An Enhanced Human Speech Emotion Recognition Using Hybrid of PRNN and KNN}, 
  year={2019},
  volume={},
  number={},
  pages={177-183},
  doi={10.1109/COMITCon.2019.8862221}}

@article{Kaufman2020,
  doi = {https://doi.org/10.1038/s41597-020-0445-3},
  url = {https://doi.org/10.1038/s41597-020-0445-3},
  year = {2020},
  month = apr,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {7},
  number = {1},
  pages = {115},
  author = {Darrell Kaufman and Nicholas McKay and Cody Routson and Michael Erb and Basil Davis and Oliver Heiri and Samuel Jaccard and Jessica Tierney and Christoph D\"{a}twyler and Yarrow Axford and Thomas Brussel and Olivier Cartapanis and Brian Chase and Andria Dawson and Anne de Vernal and Stefan Engels and Lukas Jonkers and Jeremiah Marsicek and Paola Moffa-S{\'{a}}nchez and Carrie Morrill and Anais Orsi and Kira Rehfeld and Krystyna Saunders and Philipp S. Sommer and Elizabeth Thomas and Marcela Tonello and M{\'{o}}nika T{\'{o}}th and Richard Vachula and Andrei Andreev and Sebastien Bertrand and Boris Biskaborn and Manuel Bringu{\'{e}} and Stephen Brooks and Magaly Caniup{\'{a}}n and Manuel Chevalier and Les Cwynar and Julien Emile-Geay and John Fegyveresi and Angelica Feurdean and Walter Finsinger and Marie-Claude Fortin and Louise Foster and Mathew Fox and Konrad Gajewski and Martin Grosjean and Sonja Hausmann and Markus Heinrichs and Naomi Holmes and Boris Ilyashuk and Elena Ilyashuk and Steve Juggins and Deborah Khider and Karin Koinig and Peter Langdon and Isabelle Larocque-Tobler and Jianyong Li and Andr{\'{e}} Lotter and Tomi Luoto and Anson Mackay and Eniko Magyari and Steven Malevich and Bryan Mark and Julieta Massaferro and Vincent Montade and Larisa Nazarova and Elena Novenko and Petr Pa{\v{r}}il and Emma Pearson and Matthew Peros and Reinhard Pienitz and Mateusz P{\l}{\'{o}}ciennik and David Porinchu and Aaron Potito and Andrew Rees and Scott Reinemann and Stephen Roberts and Nicolas Rolland and Sakari Salonen and Angela Self and Heikki Sepp\"{a} and Shyhrete Shala and Jeannine-Marie St-Jacques and Barbara Stenni and Liudmila Syrykh and Pol Tarrats and Karen Taylor and Valerie van den Bos and Gaute Velle and Eugene Wahl and Ian Walker and Janet Wilmshurst and Enlou Zhang and Snezhana Zhilich},
  title = {A global database of Holocene paleotemperature records},
  journal = {Scientific Data}
}

@misc{Babichev2002,
  url = {https://arxiv.org/abs/quant-ph/0208066},
  year = {2002},
  author = {S. A Babichev and J Ries and A. I Lvovsky},
  title = {Quantum scissors: teleportation of single-mode optical states by means of a nonlocal single photon},
  howpublished = {Preprint at \url{https://arxiv.org/abs/quant-ph/0208066}}
}

@ARTICLE{Figueredo:2009dg,
   author = {Figueredo, A.~J. and Wolf, P. S.~A.},
   title = {Assortative pairing and life history strategy -- a cross-cultural study},
   journal = {Human Nature},
   volume = {20},
   pages = {317-330},
   year = {2009},
   doi = {https://doi.org/10.1007/s12110-009-9068-2}
}

@MISC{Hao:gidmaps:2014,
  author = {Hao, Z. and AghaKouchak, A. and Nakhjiri, N. and Farahmand, A},
  year = {2014},
  title = {Global integrated drought monitoring and prediction system ({GIDMaPS}) data sets},
  howpublished = {\emph{figshare} \url{https://doi.org/10.6084/m9.figshare.853801}}
}

@Book{behringer2014manipulating,
 author = {Behringer, Richard},
 title = {Manipulating the mouse embryo: a laboratory manual},
 publisher = {Cold Spring Harbor Laboratory Press},
 year = {2014},
 address = {New York},
 isbn = {978-1-936113-01-9}
 }
 
 
 
 @incollection{Becker2007,
author = {Becker, Christian and Kopp, Stefan and Wachsmuth, Ipke},
booktitle = {Conversational Informatics: An Engineering Approach},
doi = {10.1002/9780470512470.ch3},
title = {{Why Emotions should be Integrated into Conversational Agents}},
year = {2007},
publisher = {John Wiley \& Sons, Ltd.}
}

@article{ball2000emotion,
  title={Emotion and personality in a conversational agent},
  author={Ball, Gene and Breese, Jack},
  journal={Embodied conversational agents},
  volume={189},
  year={2000}
}

@article{Hansen1996,
abstract = {It is well known that the introduction of acoustic background distortion and the variability resulting from environmentally induced stress causes speech recognition algorithms to fail. In this paper, several causes for recognition performance degradation are explored. It is suggested that recent studies based on a Source Generator Framework can provide a viable foundation in which to establish robust speech recognition techniques. This research encompasses three inter-related issues: (i) analysis and modeling of speech characteristics brought on by workload task stress, speaker emotion/stress or speech produced in noise (Lombard effect), (ii) adaptive signal processing methods tailored to speech enhancement and stress equalization, and (iii) formulation of new recognition algorithms which are robust in adverse environments. An overview of a statistical analysis of a Speech Under Simulated and Actual Stress (SUSAS) database is presented. This study was conducted on over 200 parameters in the domains of pitch, duration, intensity, glottal source and vocal tract spectral variations. These studies motivate the development of a speech modeling approach entitled Source Generator Framework in which to represent the dynamics of speech under stress. This framework provides an attractive means for performing feature equalization of speech under stress. In the second half of this paper, three novel approaches for signal enhancement and stress equalization are considered to address the issue of recognition under noisy stressful conditions. The first method employs (Auto:I,LSP:T) constrained iterative speech enhancement to address background noise and maximum likelihood stress equalization across formant location and bandwidth. The second method uses a feature enhancing artificial neural network which transforms the input stressed speech feature set during parameterization for keyword recognition. The final method employs morphological constrained feature enhancement to address noise and an adaptive Mel-cepstral compensation algorithm to equalize the impact of stress. Recognition performance is demonstrated for speech under a range of stress conditions, signal-to-noise ratios and background noise types.},
author = {Hansen, John H.L.},
doi = {10.1016/S0167-6393(96)00050-7},
issn = {01676393},
journal = {Speech Communication},
number = {1-2},
title = {{Analysis and compensation of speech under stress and noise for environmental robustness in speech recognition}},
volume = {20},
year = {1996}
}
@article{Zuckerman1981,
abstract = {Lying and lie detection are the two components that, together, make up the exchange called as the “communication of deception.” Deception is an act that is intended to foster in another person a belief or understanding that the deceiver considers false. This chapter presents a primarily psychological point of view and a relatively microanalysis of the verbal and nonverbal exchange between the deceiver and the lie detector. The chapter discusses the definition of deception. It describes the deceiver's perspective in lie-detection, including the strategies of deception and behaviors associated with lie-telling. The lie-detector's perspective is also discussed in the chapter, and it has described behaviors associated with the judgments of deception and strategies of lie detection. The chapter discusses the outcomes of the deceptive communication process—that is, the accuracy of lie detection—and explores methodological issues, channel effects in the detection of deception, and other factors affecting the accuracy of lie detection. {\textcopyright} 1981, Academic Press Inc.},
author = {Zuckerman, Miron and Depaulo, Bella M. and Rosenthal, Robert},
doi = {10.1016/S0065-2601(08)60369-X},
issn = {00652601},
journal = {Advances in Experimental Social Psychology},
number = {C},
title = {{Verbal and nonverbal communication of deception}},
volume = {14},
year = {1981}
}
@article{Zhou2001,
abstract = {Studies have shown that variability introduced by stress or emotion can severely reduce speech recognition accuracy. Techniques for detecting or assessing the presence of stress could help improve the robustness of speech recognition systems. Although some acoustic variables derived from linear speech production theory have been investigated as indicators of stress, they are not always consistent. In this paper, three new features derived from the nonlinear Teager energy operator (TEO) are investigated for stress classification. It is believed that the TEO based features are better able to reflect the nonlinear airflow structure of speech production under adverse stressful conditions. The features proposed include TEO-decomposed FM variation (TEO-FM-Var), normalized TEO autocorrelation envelope area (TEO-Auto-Env), and critical band based TEO autocorrelation envelope area (TEO-CB-Auto-Env). The proposed features are evaluated for the task of stress classification using simulated and actual stressed speech and it is shown that the TEO-CB-Auto-Env feature outperforms traditional pitch and mel-frequency cepstrum coefficients (MFCC) substantially. Performance for TEO based features are maintained in both text-dependent and text-independent models, while performance of traditional features degrades in text-independent models. Overall neutral versus stress classification rates are also shown to be more consistent across different stress styles.},
author = {Zhou, Guojun and Hansen, John H.L. and Kaiser, James F.},
doi = {10.1109/89.905995},
issn = {10636676},
journal = {IEEE Transactions on Speech and Audio Processing},
number = {3},
title = {{Nonlinear feature based classification of speech under stress}},
volume = {9},
year = {2001}
}
@inproceedings{Amiriparian2016,
abstract = {In this paper, we propose a method for automatically detecting deceptive speech by relying on predicted scores derived from emotion dimensions such as arousal, valence, regulation, and emotion categories. The scores are derived from task-dependent models trained on the GEMEP emotional speech database. Inputs from the INTERSPEECH 2016 Computational Paralinguistics Deception sub-challenge are processed to obtain predictions of emotion attributes and associated scores that are then used as features in detecting deception. We show that using the new emotion-related features, it is possible to improve upon the challenge baseline.},
author = {Amiriparian, Shahin and Pohjalainen, Jouni and Marchi, Erik and Pugachevskiy, Sergey and Schuller, Bj{\"{o}}rn},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
doi = {10.21437/Interspeech.2016-565},
issn = {19909772},
title = {{Is deception emotional? An emotion-driven predictive approach}},
volume = {08-12-September-2016},
year = {2016}
}
@inproceedings{Stasak2016,
abstract = {Assessing depression via speech characteristics is a growing area of interest in quantitative mental health research with a view to a clinical mental health assessment tool. As a mood disorder, depression induces changes in response to emotional stimuli, which motivates this investigation into the relationship between emotion and depression affected speech. This paper investigates how emotional information expressed in speech (i.e. arousal, valence, dominance) contributes to the classification of minimally depressed and moderately-severely depressed individuals. Experiments based on a subset of the AVEC 2014 database show that manual emotion ratings alone are discriminative of depression and combining rating-based emotion features with acoustic features improves classification between mild and severe depression. Emotion-based data selection is also shown to provide improvements in depression classification and a range of threshold methods are explored. Finally, the experiments presented demonstrate that automatically predicted emotion ratings can be incorporated into a fully automatic depression classification to produce a 5% accuracy improvement over an acoustic-only baseline system.},
author = {Stasak, Brian and Epps, Julien and Cummins, Nicholas and Goecke, Roland},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
doi = {10.21437/Interspeech.2016-867},
issn = {19909772},
title = {{An investigation of emotional speech in depression classification}},
volume = {08-12-September-2016},
year = {2016}
}
@article{Low2011,
abstract = {The properties of acoustic speech have previously been investigated as possible cues for depression in adults. However, these studies were restricted to small populations of patients and the speech recordings were made during patients' clinical interviews or fixed-text reading sessions. Symptoms of depression often first appear during adolescence at a time when the voice is changing, in both males and females, suggesting that specific studies of these phenomena in adolescent populations are warranted. This study investigated acoustic correlates of depression in a large sample of 139 adolescents (68 clinically depressed and 71 controls). Speech recordings were made during naturalistic interactions between adolescents and their parents. Prosodic, cepstral, spectral, and glottal features, as well as features derived from the Teager energy operator (TEO), were tested within a binary classification framework. Strong gender differences in classification accuracy were observed. The TEO-based features clearly outperformed all other features and feature combinations, providing classification accuracy ranging between 81%-87%- for males and 72%-79%- for females. Close, but slightly less accurate, results were obtained by combining glottal features with prosodic and spectral features (67%-69%- for males and 70%-75%- for females). These findings indicate the importance of nonlinear mechanisms associated with the glottal flow formation as cues for clinical depression. {\textcopyright} 2006 IEEE.},
author = {Low, Lu Shih Alex and Maddage, Namunu C. and Lech, Margaret and Sheeber, Lisa B. and Allen, Nicholas B.},
doi = {10.1109/TBME.2010.2091640},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
number = {3 PART 1},
title = {{Detection of clinical depression in adolescents' speech during family interactions}},
volume = {58},
year = {2011}
}
@inproceedings{Tokuno2011,
abstract = {Post Traumatic Stress Disorder (PTSD), depression and suicide are major psychiatric problem in both military and civilian situation. These mental diseases combine with emotion change. Recently, the technology of emotion recognition has been developed rapidly and highly. Therefore, we investigate if the emotion recognition by natural speaking voice could detect the emotion change which would occur when exposing mental stress. We used Sensibility technology ST Emotion (AGI Japan Inc.) for emotion voice analysis system. This system determines emotional elements as including anger, joy, sorrow, and calmness. It also measures feeling of excitement. Voice data were collected from the personnel of military medical corps participating in a special stressful mission. The voice data were divided into two groups depending on participating period. Some subject's feelings during experimental period were changed. There is a tendency that joy of long stay group (Group L) is lower than short stay group (Group S) and sorrow of Group L is higher than Group S. The result suggested that the techniques of emotion recognition may be used for screening of mental status in military situation. However, further development is necessary for practical use. {\textcopyright} 2011 IEEE.},
author = {Tokuno, Shinichi and Tsumatori, Gentaro and Shono, Satoshi and Takei, Eriko and Yamamoto, Taisuke and Suzuki, Go and Mituyoshi, Shunnji and Shimura, Makoto},
booktitle = {2011 Defense Science Research Conference and Expo, DSR 2011},
doi = {10.1109/DSR.2011.6026823},
title = {{Usage of emotion recognition in military health care}},
year = {2011}
}
@inproceedings{Tacconi2008,
abstract = {Diagnosis of psychiatric diseases is currently accomplished with questionnaires tilled in by the subjects, usually together with a specialist Such questionnaires are often based on standard, formal scales, where questions range from the ability to cope with household activities, through social interactions, agitation, level of activity, to quality of sleep. In this paper, we propose a context aware framework to support semi-automation of the diagnosis of such diseases, leveraging on authors experience in the fields of activity and emotion recognition. We also introduce a brief description of the main components defining the architecture of the proposed platform and preliminary work on emotion estimation.},
author = {Tacconi, David and Mayora, Oscar and Lukowicz, Paul and Arnrich, Bert and Setz, Cornelia and Tr{\"{o}}ster, Gerhard and Haring, Christian},
booktitle = {Proceedings of the 2nd International Conference on Pervasive Computing Technologies for Healthcare 2008, PervasiveHealth},
doi = {10.1109/PCTHEALTH.2008.4571041},
title = {{Activity and emotion recognition to support early diagnosis of psychiatric diseases}},
year = {2008}
}


@article{Mano2016,
abstract = {Currently, there is an increasing number of patients that are treated in-home, mainly in countries such as Japan, USA and Europe. As well as this, the number of elderly people has increased significantly in the last 15 years and these people are often treated in-home and at times enter into a critical situation that may require help (e.g. when facing an accident, or becoming depressed). Advances in ubiquitous computing and the Internet of Things (IoT) have provided efficient and cheap equipments that include wireless communication and cameras, such as smartphones or embedded devices like Raspberry Pi. Embedded computing enables the deployment of Health Smart Homes (HSH) that can enhance in-home medical treatment. The use of camera and image processing on IoT is still an application that has not been fully explored in the literature, especially in the context of HSH. Although use of images has been widely exploited to address issues such as safety and surveillance in the house, they have been little employed to assist patients and/or elderly people as part of the home-care systems. In our view, these images can help nurses or caregivers to assist patients in need of timely help, and the implementation of this application can be extremely easy and cheap when aided by IoT technologies. This article discusses the use of patient images and emotional detection to assist patients and elderly people within an in-home healthcare context. We also discuss the existing literature and show that most of the studies in this area do not make use of images for the purpose of monitoring patients. In addition, there are few studies that take into account the patient's emotional state, which is crucial for them to be able to recover from a disease. Finally, we outline our prototype which runs on multiple computing platforms and show results that demonstrate the feasibility of our approach.},
author = {Mano, Leandro Y. and Fai{\c{c}}al, Bruno S. and Nakamura, Luis H.V. and Gomes, Pedro H. and Libralon, Giampaolo L. and Meneguete, Rodolfo I. and Filho, Geraldo P.R. and Giancristofaro, Gabriel T. and Pessin, Gustavo and Krishnamachari, Bhaskar and Ueyama, J{\'{o}}},
doi = {10.1016/j.comcom.2016.03.010},
issn = {1873703X},
journal = {Computer Communications},
title = {{Exploiting IoT technologies for enhancing Health Smart Homes through patient identification and emotion recognition}},
volume = {89-90},
year = {2016}
}


@article{russell1979affective,
  title={Affective space is bipolar.},
  author={Russell, James A},
  journal={Journal of personality and social psychology},
  volume={37},
  number={3},
  pages={345},
  year={1979},
  publisher={American Psychological Association}
}

@INPROCEEDINGS{4607572,

  author={Grimm, Michael and Kroschel, Kristian and Narayanan, Shrikanth},

  booktitle={2008 IEEE International Conference on Multimedia and Expo}, 

  title={The Vera am Mittag German audio-visual emotional speech database}, 

  year={2008},

  volume={},

  number={},

  pages={865-868},

  doi={10.1109/ICME.2008.4607572}}


@misc{ser_datasets,
author = {Ayoub~Malek},
title = {SER-datasets: A collection of datasets for the purpose of emotion recognition/detection in speech.},
howpublished = {\url{https://github.com/SuperKogito/SER-datasets}},
month = {},
year = {2021},
note = {(Accessed on 04/08/2022)}
}

@article{ververidis2006emotional,
  title={Emotional speech recognition: Resources, features, and methods},
  author={Ververidis, Dimitrios and Kotropoulos, Constantine},
  journal={Speech communication},
  volume={48},
  number={9},
  pages={1162--1181},
  year={2006},
  publisher={Elsevier}
}

@article{swain2018databases,
  title={Databases, features and classifiers for speech emotion recognition: a review},
  author={Swain, Monorama and Routray, Aurobinda and Kabisatpathy, Prithviraj},
  journal={International Journal of Speech Technology},
  volume={21},
  number={1},
  pages={93--120},
  year={2018},
  publisher={Springer}
}

@article{byun2021study,
  title={A Study on a Speech Emotion Recognition System with Effective Acoustic Features Using Deep Learning Algorithms},
  author={Byun, Sung-Woo and Lee, Seok-Pil},
  journal={Applied Sciences},
  volume={11},
  number={4},
  pages={1890},
  year={2021},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@misc{q1,
    Author = "{Ministerio de Cultura de Perú}",
    Title  = "Quechua",
    note   = {\url{https://bdpi.cultura.gob.pe/lenguas/quechua}, accedido en  15/03/2022},
    year   = 2018
}

@article{lrl,
  title={Low-resource languages: A review of past work and future challenges},
  author={Magueresse, Alexandre and Carles, Vincent and Heetderks, Evan},
  journal={arXiv preprint arXiv:2006.07264},
  year={2020}
}

@book{q3,
author={Mehrabian, A. and Russell, J. A.},
title={An approach to environmental psychology},
year={1974},
publisher={The
Massachusetts Institute of Technology},
address={USA}
}

%%%%%%%%%%%%%%%%%%%%%%
@ARTICLE{q2,

  author={Nassif, Ali Bou and Shahin, Ismail and Attili, Imtinan and Azzeh, Mohammad and Shaalan, Khaled},

  journal={IEEE Access}, 

  title={Speech Recognition Using Deep Neural Networks: A Systematic Review}, 

  year={2019},

  volume={7},

  number={},

  pages={19143-19165},

  doi={10.1109/ACCESS.2019.2896880}}


@article{q4,
author={Camacho, L. and Zevallos, R. and Cárdenas, R. and Baquerizo, R.},
title={ Siminchik: A Speech Corpus for Preservation of Southern Quechua},
year={2018},
journal={Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC'18)}
}

@inproceedings{q5,
author = {Camacho, Luis and Vergara, Melgarejo and Zevallos, Rodolfo},
year = {2017},
month = {12},
booktitle = {Latin American and Iberian Languages
  Open Corpora Forum},
title = {On the Building of the Large Scale Corpus of Southern Qichwa}
}

@article{q6,
author={Chacca, H. and Montufar, R. and Gonzales, J.},
title={Isolated Automatic Speech Recognition of Quechua Numbers using {MFCC}, {DTW} and {KNN}},
year={2018},
journal={International Journal of Advanced Computer Science and Applications},
volume={9},
number={10}
}

@article{q7,
author = {Seo, Yeong-Seok and Huh, Jun-Ho},
year = {2019},
month = {02},
pages = {164},
title = {Automatic Emotion-Based Music Classification for Supporting Intelligent IoT Applications},
volume = {8},
journal = {Electronics},
doi = {10.3390/electronics8020164}
}

@article{q8,
author = {Russell, J.},
year = {1980},
title = {A Circumplex Model of Affect},
volume = {39},
journal = {Journal of Personality and Social Psychology},
number={6},
pages={1161--1178}
}

@article{q9,
author = {Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower Provost, Emily and Kim, Samuel and Chang, Jeannette and Lee, Sungbok and Narayanan, Shrikanth},
year = {2008},
month = {12},
pages = {335-359},
title = {IEMOCAP: Interactive emotional dyadic motion capture database},
volume = {42},
journal = {Language Resources and Evaluation},
doi = {10.1007/s10579-008-9076-6}
}

@article{q10,
author = {Eyben, Florian and Scherer, Klaus and Schuller, Björn and Sundberg, Johan and Andre, Elisabeth and Busso, Carlos and Devillers, Laurence and Epps, Julien and Laukka, Petri and Narayanan, Shrikanth and Truong, Khiet},
year = {2015},
month = {01},
pages = {1-1},
title = {The {Geneva Minimalistic Acoustic Parameter Set (GeMAPS)} for Voice Research and Affective Computing},
volume = {7},
journal = {IEEE Transactions on Affective Computing},
doi = {10.1109/TAFFC.2015.2457417}
}

@INPROCEEDINGS{q11,  author={Guzman, Yudi and Tavara, Alexis and Zevallos, Rodolfo and Vega, Hugo},  booktitle={2021 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)},   title={Implementation of a Bilingual Participative Argumentation Web Platform for collection of Spanish Text and Quechua Speech},   year={2021},  volume={},  number={},  pages={1-6},  doi={10.1109/ICECCE52056.2021.9514251}}

@book{sam,
author={Fischer, L. and Brauns, D. and Belschak, F.},
year={2002},
booktitle={Zur Messung von Emotionen in der angewandten Forschung},
publisher={Pabst Science Publishers},
address={Lengerich}
}

@article{scikit,
author = {Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'{E}douard},
title = {Scikit-Learn: Machine Learning in Python},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
journal = {J. Mach. Learn. Res.},
month = {nov},
pages = {2825–2830},
numpages = {6}
}

@INPROCEEDINGS{bagus,  author={Atmaja, Bagus Tris and Akagi, Masato},  booktitle={2020 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)},   title={Deep Multilayer Perceptrons for Dimensional Speech Emotion Recognition},   year={2020},  volume={},  number={},  pages={325-331},  doi={}}

@article{serreview,
author = {Schuller, Bj\"{o}rn W.},
title = {Speech Emotion Recognition: Two Decades in a Nutshell, Benchmarks, and Ongoing Trends},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3129340},
doi = {10.1145/3129340},
abstract = {Tracing 20 years of progress in making machines hear our emotions based on speech signal properties.},
journal = {Commun. ACM},
month = {apr},
pages = {90–99},
numpages = {10}
}

@inproceedings{c1,
  title={Multitask learning and multistage fusion for dimensional audiovisual emotion recognition},
  author={Atmaja, Bagus Tris and Akagi, Masato},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4482--4486},
  year={2020},
  organization={IEEE}
}

@INPROCEEDINGS{c2,  author={Han, Jing and Zhang, Zixing and Ringeval, Fabien and Schuller, Björn},  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Prediction-based learning for continuous emotion recognition in speech},   year={2017},  volume={},  number={},  pages={5005-5009},  doi={10.1109/ICASSP.2017.7953109}}

@inproceedings{c3,
  title={Emotion recognition using fusion of audio and video features},
  author={Ortega, Juan DS and Cardinal, Patrick and Koerich, Alessandro L},
  booktitle={2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)},
  pages={3847--3852},
  year={2019},
  organization={IEEE}
}


@article{Lotfian2019,
   abstract = {The lack of a large, natural emotional database is one of the key barriers to translate results on speech emotion recognition in controlled conditions into real-life applications. Collecting emotional databases is expensive and time demanding, which limits the size of existing corpora. Current approaches used to collect spontaneous databases tend to provide unbalanced emotional content, which is dictated by the given recording protocol (e.g., positive for colloquial conversations, negative for discussion or debates). The size and speaker diversity are also limited. This paper proposes a novel approach to effectively build a large, naturalistic emotional database with balanced emotional content, reduced cost and reduced manual labor. It relies on existing spontaneous recordings obtained from audio-sharing websites. The proposed approach combines machine learning algorithms to retrieve recordings conveying balanced emotional content with a cost effective annotation process using crowdsourcing, which make it possible to build a large scale speech emotional database. This approach provides natural emotional renditions from multiple speakers, with different channel conditions and conveying balanced emotional content that are difficult to obtain with alternative data collection protocols.},
   author = {Reza Lotfian and Carlos Busso},
   doi = {10.1109/TAFFC.2017.2736999},
   issn = {19493045},
   issue = {4},
   journal = {IEEE Transactions on Affective Computing},
   title = {Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech from Existing Podcast Recordings},
   volume = {10},
   year = {2019},
}

@article{ccc,
title={A concordance correlation coefficient to evaluate reproducibility},
journal={Biometrics},
volume={45},
year={1989},
pages={255--268},
author={Lin, L. I.},
number={1}
}
















@article{Zvarevashe2020,
   abstract = {Most of the studies on speech emotion recognition have used single-language corpora, but little research has been done in cross-language valence speech emotion recognition. Research has shown that the models developed for single-language speech recognition systems perform poorly when used in different environments. Cross-language speech recognition is a craving alternative, but it is highly challenging because the corpora used will have been recorded in different environments and under varying conditions. The differences in the quality of recording devices, elicitation techniques, languages, and accents of speakers make the recognition task even more arduous. In this paper, we propose a stacked ensemble learning algorithm to recognize valence emotion in a cross-language speech environment. The proposed ensemble algorithm was developed from random decision forest, AdaBoost, logistic regression, and gradient boosting machine and is therefore called RALOG. In addition, we propose feature scaling using random forest recursive feature elimination and a feature selection algorithm to boost the performance of RALOG. The algorithm has been evaluated against four widely used ensemble algorithms to appraise its performance. The amalgam of five benchmarked corpora has resulted in a cross-language corpus to validate the performance of RALOG trained with the selected acoustic features. The comparative analysis results have shown that RALOG gave better performance than the other ensemble learning algorithms investigated in this study.},
   author = {Kudakwashe Zvarevashe and Oludayo O. Olugbara},
   doi = {10.3390/a13100246},
   issue = {10},
   journal = {Algorithms},
   title = {Recognition of Cross-Language Acoustic Emotional Valence Using Stacked Ensemble Learning},
   volume = {13},
   year = {2020},
}

@article{Maithri2022,
   author = {M. Maithri and U. Raghavendra and Anjan Gudigar and Jyothi Samanth and Prabal Datta Barua and Murugappan Murugappan and Yashas Chakole and U. Rajendra Acharya},
   doi = {10.1016/j.cmpb.2022.106646},
   issn = {18727565},
   journal = {Computer Methods and Programs in Biomedicine},
   title = {Automated emotion recognition: Current trends and future perspectives},
   volume = {215},
   year = {2022}
}


@inproceedings{Deshpande2019,
   author = {Gauri Deshpande and Venkata Subramanian Viraraghavan and Mayuri Duggirala and Sachin Patel},
   doi = {10.1109/EMBC.2019.8857691},
   issn = {1557170X},
   booktitle = {Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
   title = {Detecting emotional valence using time-domain analysis of speech signals},
   year = {2019}
}

@article{Kossaifi2021,
   abstract = {Natural human-computer interaction and audio-visual human behaviour sensing systems, which would achieve robust performance in-the-wild are more needed than ever as digital devices are increasingly becoming an indispensable part of our life. Accurately annotated real-world data are the crux in devising such systems. However, existing databases usually consider controlled settings, low demographic variability, and a single task. In this paper, we introduce the SEWA database of more than 2,000 minutes of audio-visual data of 398 people coming from six cultures, 50 percent female, and uniformly spanning the age range of 18 to 65 years old. Subjects were recorded in two different contexts: while watching adverts and while discussing adverts in a video chat. The database includes rich annotations of the recordings in terms of facial landmarks, facial action units (FAU), various vocalisations, mirroring, and continuously valued valence, arousal, liking, agreement, and prototypic examples of (dis)liking. This database aims to be an extremely valuable resource for researchers in affective computing and automatic human sensing and is expected to push forward the research in human behaviour analysis, including cultural studies. Along with the database, we provide extensive baseline experiments for automatic FAU detection and automatic valence, arousal, and (dis)liking intensity estimation.},
   author = {Jean Kossaifi and Robert Walecki and Yannis Panagakis and Jie Shen and Maximilian Schmitt and Fabien Ringeval and Jing Han and Vedhas Pandit and Antoine Toisoul and Bjorn Schuller and Kam Star and Elnar Hajiyev and Maja Pantic},
   doi = {10.1109/TPAMI.2019.2944808},
   issn = {19393539},
   issue = {3},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   title = {SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild},
   volume = {43},
   year = {2021},
}

@inproceedings{catania2019automatic,
  title={Automatic Speech Recognition: Do Emotions Matter?},
  author={Catania, Fabio and Crovari, Pietro and Spitale, Micol and Garzotto, Franca},
  booktitle={2019 IEEE International Conference on Conversational Data \& Knowledge Engineering (CDKE)},
  pages={9--16},
  year={2019},
  organization={IEEE}
}

@article{ps2017emotion,
  title={Emotion models: a review},
  author={PS, Sreeja and Mahalakshmi, G},
  journal={International Journal of Control Theory and Applications},
  volume={10},
  pages={651--657},
  year={2017}
}

@article{yamashita2013review,
  title={A review of paralinguistic information processing for natural speech communication},
  author={Yamashita, Yoichi},
  journal={Acoustical Science and Technology},
  volume={34},
  number={2},
  pages={73--79},
  year={2013},
  publisher={Acoustical Society of Japan}
}


@book{schuller2013computational,
  title={Computational paralinguistics: emotion, affect and personality in speech and language processing},
  author={Schuller, Bj{\"o}rn and Batliner, Anton},
  year={2013},
  publisher={John Wiley \& Sons}
}

@article{venkataramanan2019emotion,
  title={Emotion recognition from speech},
  author={Venkataramanan, Kannan and Rajamohan, Haresh Rengaraj},
  journal={arXiv preprint arXiv:1912.10458},
  year={2019}
}

@inproceedings{recola,
abstract = {We present in this paper a new multimodal corpus of spontaneous collaborative and affective interactions in French: RECOLA, which is being made available to the research community. Participants were recorded in dyads during a video conference while completing a task requiring collaboration. Different multimodal data, i.e., audio, video, ECG and EDA, were recorded continuously and synchronously. In total, 46 participants took part in the test, for which the first 5 minutes of interaction were kept to ease annotation. In addition to these recordings, 6 annotators measured emotion continuously on two dimensions: arousal and valence, as well as social behavior labels on five dimensions. The corpus allowed us to take self-report measures of users during task completion. Methodologies and issues related to affective corpus construction are briefly reviewed in this paper. We further detail how the corpus was constructed, i.e., participants, procedure and task, the multimodal recording setup, the annotation of data and some analysis of the quality of these annotations. {\textcopyright} 2013 IEEE.},
author = {Ringeval, Fabien and Sonderegger, Andreas and Sauer, Juergen and Lalanne, Denis},
booktitle = {2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2013},
doi = {10.1109/FG.2013.6553805},
title = {{Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions}},
year = {2013}
}
@article{semaine,
abstract = {SEMAINE has created a large audiovisual database as a part of an iterative approach to building Sensitive Artificial Listener (SAL) agents that can engage a person in a sustained, emotionally colored conversation. Data used to build the agents came from interactions between users and an operator simulating a SAL agent, in different configurations: Solid SAL (designed so that operators displayed an appropriate nonverbal behavior) and Semi-automatic SAL (designed so that users' experience approximated interacting with a machine). We then recorded user interactions with the developed system, Automatic SAL, comparing the most communicatively competent version to versions with reduced nonverbal skills. High quality recording was provided by five high-resolution, high-framerate cameras, and four microphones, recorded synchronously. Recordings total 150 participants, for a total of 959 conversations with individual SAL characters, lasting approximately 5 minutes each. Solid SAL recordings are transcribed and extensively annotated: 6-8 raters per clip traced five affective dimensions and 27 associated categories. Other scenarios are labeled on the same pattern, but less fully. Additional information includes FACS annotation on selected extracts, identification of laughs, nods, and shakes, and measures of user engagement with the automatic system. The material is available through a web-accessible database. {\textcopyright} 2010-2012 IEEE.},
author = {McKeown, Gary and Valstar, Michel and Cowie, Roddy and Pantic, Maja and Schr{\"{o}}der, Marc},
doi = {10.1109/T-AFFC.2011.20},
issn = {19493045},
journal = {IEEE Transactions on Affective Computing},
number = {1},
title = {{The SEMAINE database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent}},
volume = {3},
year = {2012}
}
@inproceedings{semour,
abstract = {Designing reliable Speech Emotion Recognition systems is a complex task that inevitably requires sufcient data for training purposes. Such extensive datasets are currently available in only a few languages, including English, German, and Italian. In this paper, we present SEMOUR, the frst scripted database of emotion-tagged speech in the Urdu language, to design an Urdu Speech Recognition System. Our gender-balanced dataset contains 15, 040 unique instances recorded by eight professional actors eliciting a syntactically complex script. The dataset is phonetically balanced, and reliably exhibits a varied set of emotions as marked by the high agreement scores among human raters in experiments. We also provide various baseline speech emotion prediction scores on the database, which could be used for various applications like personalized robot assistants, diagnosis of psychological disorders, and getting feedback from a low-tech-enabled population, etc. On a random test sample, our model correctly predicts an emotion with a state-of-the-art 92% accuracy.},
author = {Zaheer, Nimra and Ahmad, Obaid Ullah and Ahmed, Ammar and Khan, Muhammad Shehryar and Shabbir, Mudassir},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/3411764.3445171},
title = {{Semour: A scripted emotional speech repository for urdu}},
year = {2021}
}

@article{musecar,
abstract = {Truly real-life data presents a strong, but exciting challenge for sentiment and emotion research. The high variety of possible &#x2018;in-the-wild&#x2019; properties makes large datasets such as these indispensable with respect to building robust machine learning models. A sufficient quantity of data covering a deep variety in the challenges of each modality to force the exploratory analysis of the interplay of all modalities has not yet been made available in this context. In this contribution, we present MuSe-CaR, a first of its kind multimodal dataset. The data is publicly available as it recently served as the testing bed for the 1st Multimodal Sentiment Analysis Challenge, and focused on the tasks of emotion, emotion-target engagement, and trustworthiness recognition by means of comprehensively integrating the audio-visual and language modalities. Furthermore, we give a thorough overview of the dataset in terms of collection and annotation, including annotation tiers not used in this year's MuSe 2020. In addition, for one of the sub-challenges -- predicting the level of trustworthiness -- no participant outperformed the baseline model, and so we propose a simple, but highly efficient Multi-Head-Attention network that exceeds using multimodal fusion the baseline by around 0.2 CCC (almost 50 % improvement).},
author = {Stappen, Lukas and Baird, Alice and Schumann, Lea and Bjorn, Schuller},
doi = {10.1109/TAFFC.2021.3097002},
issn = {19493045},
journal = {IEEE Transactions on Affective Computing},
title = {{The Multimodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset: Collection, Insights and Improvements}},
year = {2021}
}

@article{anpst,
author = {Imbir, Kamil},
year = {2016},
month = {07},
pages = {1030},
title = {Affective Norms for 718 Polish Short Texts (ANPST): Dataset with Affective Ratings for Valence, Arousal, Dominance, Origin, Subjective Significance and Source Dimensions},
volume = {7},
journal = {Frontiers in Psychology},
doi = {10.3389/fpsyg.2016.01030}
}



%%%%%%%%%%%lstm


@article{soma2015simultaneous,
  title={Simultaneous multichannel signal transfers via chaos in a recurrent neural network},
  author={Soma, Ken-ichiro and Mori, Ryota and Sato, Ryuichi and Furumai, Noriyuki and Nara, Shigetoshi},
  journal={Neural computation},
  volume={27},
  number={5},
  pages={1083--1101},
  year={2015},
  publisher={MIT Press}
}

@article{linzen2016assessing,
  title={Assessing the ability of LSTMs to learn syntax-sensitive dependencies},
  author={Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  journal={arXiv preprint arXiv:1611.01368},
  year={2016}
}
@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}


@article{22,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press},
}

@article{LC30,
  title={Distance metric learning for large margin nearest neighbor classification},
  author={Weinberger, Kilian Q and Saul, Lawrence K},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Feb},
  pages={207--244},
  year={2009}
}


@article{lc31qian2015comparing,
  title={Comparing machine learning classifiers for object-based land cover classification using very high resolution imagery},
  author={Qian, Yuguo and Zhou, Weiqi and Yan, Jingli and Li, Weifeng and Han, Lijian},
  journal={Remote Sensing},
  volume={7},
  number={1},
  pages={153--168},
  year={2015},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{lc35breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}
@article{mountrakis,
  title={Support vector machines in remote sensing: A review},
  author={Mountrakis, Giorgos and Im, Jungho and Ogole, Caesar},
  journal={ISPRS Journal of Photogrammetry and Remote Sensing},
  volume={66},
  number={3},
  pages={247--259},
  year={2011},
  publisher={Elsevier}
}

@book{steinwart,
  title={Support vector machines},
  author={Steinwart, Ingo and Christmann, Andreas},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@phdthesis{myburgh,
  title={The impact of training set size and feature dimensionality on supervised object-based classification: a comparison of three classifiers},
  author={Myburgh, Gerhard},
  year={2012},
  school={Stellenbosch: Stellenbosch University}
}

@article{suykens,
  title={Least squares support vector machine classifiers},
  author={Suykens, Johan AK and Vandewalle, Joos},
  journal={Neural processing letters},
  volume={9},
  number={3},
  pages={293--300},
  year={1999},
  publisher={Springer}
}



%---

@article{AMEIJEIRASSANCHEZ2020,
   title = {{Revision de algoritmos de deteccion y seguimiento de objetos con redes profundas para videovigilancia inteligente}},
   journal = {{Revista Cubana de Ciencias Informáticas}},
   author={Ameijeiras Sanchez, David AND Gonzalez Diez, H. R. AND Hernandez Heredia, Yanio},
   ISSN = {2227-1899},
   language = {es},
   URL = {http://scielo.sld.cu/scielo.php?script=sci_arttext&pid=S2227-18992020000300165&nrm=iso},
   volume = {14},
   year = {2020},
   month = {09},
   pages = {165 - 195},
   publisher = {scielocu},
   }
   
@inproceedings{feeltrace,
author = {Cowie, Roddy and Douglas-Cowie, E. and Savvidou, Suzie and McMahon, E. and Sawey, M. and Schr\"oder, M.},
year = {2000},
month = {01},
title = {'FEELTRACE': An instrument for recording perceived emotion in real time},
booktitle = {Proceedings of the ISCA Workshop on Speech and Emotion}
}

@book{text1, place={Cusco, Perú}, edition={2}, title={Gramática Quechua }, volume={1}, publisher={Centro Bartolomé de las Casas}, author={Cusihuamán G., Antonio}, year={2001}} 
 
 @book{text2, place={Santiago de Chile, Chile}, title={Guías pedagógicas del sector lengua indígena - Quechua - material de apoyo para la enseñanza}, publisher={Ministerio de Educación de Chile}, author={Herrera, Alma and Quispe, Julia and Cariman, Alejandra and Fuentes, Claudio and Contreras, Daniel and Cortés, Soledad}, year={2012}} 
 
 
  @book{text3, title={Qullaw qichwapa simi qullqan: A-Y}, address={Lima, Perú}, author={Chuqumamani Valer, Nonato Rufino and  Alosilla Morales, Carmen Gladis and  Choque Valer, Victoria}, year={2014}, publisher={Ministerio de Educación}} 
  
  
    @book{text4, place={Lima, Perú}, edition={1}, title={Manual para el empleo del Quechua Cusco Collao en la administración de justicia}, volume={1}, publisher={Ministerio de Cultura}, author={Aranda Escalante, Mirva}, year={2015}} 
    
@book{text5, place={Lima, Perú}, edition={1}, title={Runa simi qillqay yachana mayt’u},  publisher={Ministerio de Cultura}, author={ Hancco Mamani, Nereo Aquiles and  Arisaca Mamani, Juana and  Huahuatico Espinoza, Guillermo and
 Quispe Puma, Rafael Jaime and  Layme Narvaez, Brígida and  Huacoto Bejar, Juana and 
Meléndez Arce, Armando and  Pino Apaza, Eulalia Antonia and  Espinoza Ramos, Marcelino and 
Valeriano Aguilar, Miguel Angel and  Calcina Tito, Luis Marino and  Sullca Quispe, Godofredo Victoriano and 
Sánchez Sánchez, Dámaso and  Serrano Bustinza, Alexander and  Gamarra Saidivar, Basilia and  Otazú
Livón, Teresa and  Huamán Julluni, Shara}, year={2015}} 
  
  @book{text6, title={{Didáctica Quechua I -
DRE Apurímac EBI-Rural}}, author={ Pinto Tapia, Miguel Angel}, publisher={Direcciòn Regional de Educación de Apurímac}, year={2005}} 

 
@book{text7, title={Palabras útiles en el quechua de Caylloma}, author={Kindberg, Eric and de Kindberg, Kary Lynn}, year={1985},
publisher={Instituto Lingüístico de Verano}} 

@book{text8,
title={Kuska yachasunchik. 
Cuaderno de trabajo y folder - inicial 4 años Quechua Collao},
author={Sullca Peña, Adela},
address={Lima, Perú},
edition={4},
year={2020},
publisher={Ministerio de Educación}
}
 
 @book{text9, title={Yachakuqkunapa Simi Qullqa - Qusqu Qullaw Qhichwa Simipi}, publisher={Ministerio de Educación}, author={ Chuquimamani Valer, Nonato Rufino}, year={2005}} 
 
 @book{text10, title={Simikunapi, Kawsaykunapi, Sapsikunapi Tukuy niraq Yachachina Umalliq  Iskay Simipi Kawsaypura Yachachiy Umalliq}, publisher={Gráfica Biblos S.A.}, author={Llamoja Tapia, Marina}, year={2021}} 
 
 @book{text11, title={Kunan punchaw  runasimita istudyasaq}, author={Ortiz Vásquez, Ricardo}, year={2017}, publisher={Universidad Nacional Mayor de San Marcos}} 
 
 @book{text12, title={Qhechua de Cusco - Collao }, author={Rodríguez, Albino}, year={2021}, address={Lima, Perú}} 
 
 @book{text13, title={Qayna, kunan, paqarin.  Una introducción práctica al quechua chanca}, publisher={Pontificia Universidad Católica del Perú}, author={Zariquiey, Roberto and Córdova, Gavina}, year={2008}} 
 
 @book{text14,
 title={Manual de gramática Quechua Cusco-Collao},
 author={Cahuana Q., Ricardo},
 address={Sicuani, Perú},
 year={2007}
 }
 
 @book{text15,
 year={2003},
 title={Huchuy Pumacha - Pumita},
 address = {Cusco, Perú},
 publisher={Instituto Nacional de Cultura},
 author={{Instituto Nacional de Cultura}}
 }
 
 @inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
} }

@inbook {keltner2010,
	title = {Emotion},
	booktitle = {The Handbook of Social Psychology},
	year = {2010},
	pages = {317-352},
	publisher = {Wiley},
	organization = {Wiley},
	address = {New York},
	author = {Keltner, D. and Lerner, J.S.}
}

@article{ekman1992,
author = { Paul   Ekman },
title = {An argument for basic emotions},
journal = {Cognition and Emotion},
volume = {6},
number = {3-4},
pages = {169-200},
year  = {1992},
publisher = {Routledge},
doi = {10.1080/02699939208411068},

URL = { 
        https://doi.org/10.1080/02699939208411068
    
},
eprint = { 
        https://doi.org/10.1080/02699939208411068
    
}

}

@article{reviewemo,
author = {P S, SREEJA and G S, Mahalakshmi},
year = {2017},
month = {01},
pages = {651-657},
title = {Emotion Models: A Review},
volume = {10},
journal = {International Journal of Control Theory and Applications}
}

@article{bakker,
author = {Bakker, Iris and Van der Voordt, Theo and Boon, Jan and Vink, Peter},
year = {2014},
month = {10},
pages = {405-421},
title = {Pleasure, Arousal, Dominance: Mehrabian and Russell revisited},
volume = {33},
journal = {Current Psychology},
doi = {10.1007/s12144-014-9219-4}
}

@book{bakkerboon,
author={Bakker, I. C. and  de Boon J. C.},
year={2012},
title={Zorg voor mens en omgeving, Het zintuig als maatstaf},
publisher={KCWZ},
address={Utretch},
pages={84–-89}
}

@book{dnlo,
author={{Ministerio de Educación}},
year={2013},
title={Documento Nacional de Lenguas Originarias (DNLO)},
publisher={Ministerio de Educación},
address={Lima},
url={https://centroderecursos.cultura.pe/es/registrobibliografico/documento-nacional-de-lenguas-originarias-del-perú}
}

@InProceedings{qst,
author="Zevallos, Rodolfo
and Cordova, Johanna
and Camacho, Luis",
editor="Lossio-Ventura, Juan Antonio
and Condori-Fernandez, Nelly
and Valverde-Rebaza, Jorge Carlos",
title="Automatic Speech Recognition of Quechua Language Using HMM Toolkit",
booktitle="Information Management and Big Data",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="61--68",
abstract="In this paper, we present the implementation of an Automatic Speech Recognition system (ASR) for southern Quechua language. The software can recognize both continuous speech and isolated words. The ASR was developed using Hidden Markov Model Toolkit (HTK) and the corpus collected by Siminchikkunarayku. A dictionary provides the system with a mapping of vocabulary words to sequences of phonemes; the audio files were processed to extract the speech feature vectors (MFCC) and then, the acoustic model was trained using the MFCC files until its convergence. The paper also describes a detailed architecture of an ASR system developed using HTK library modules and tools. The ASR was tested using the audios recorded by volunteers obtaining a 12.70{\%} word error rate.",
isbn="978-3-030-46140-9"
}

@book{qst2,
title={Urin qichwa qillqay yachana mayt'u. Manual de escritura
quechua sureño},
author={{Ministerio de Educación}},
year={2021},
address={Lima}
}


@incollection{batliner2011automatic,
  title={The automatic recognition of emotions in speech},
  author={Batliner, Anton and Schuller, Bj{\"o}rn and Seppi, Dino and Steidl, Stefan and Devillers, Laurence and Vidrascu, Laurence and Vogt, Thurid and Aharonson, Vered and Amir, Noam},
  booktitle={Emotion-Oriented Systems},
  pages={71--99},
  year={2011},
  publisher={Springer}
}


@Article{electronics8020164,
AUTHOR = {Seo, Yeong-Seok and Huh, Jun-Ho},
TITLE = {Automatic Emotion-Based Music Classification for Supporting Intelligent IoT Applications},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {2},
ARTICLE-NUMBER = {164},
URL = {https://www.mdpi.com/2079-9292/8/2/164},
ISSN = {2079-9292},
DOI = {10.3390/electronics8020164}
}
